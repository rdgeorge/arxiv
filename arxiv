#!/usr/bin/env python

"""
This script queries arxiv for the latest papers and prints the most relevant,
based on a keyword search of their titles.
"""

from __future__ import print_function
import argparse
import datetime
from os.path import expanduser
import re
try:
    import urllib.request as req
except ImportError:
    import sys
    reload(sys)  # Hack to put setdefaultencoding back in after python startup
    sys.setdefaultencoding("utf-8")  # Output utf-8 to terminal
    import urllib2 as req
import xml.etree.ElementTree as ET


def title_relevance(title, keywords):
    """
    Return a float denoting the relevance of a string given keywords

    Parameters
    ----------
    title : string
        The string to determine a relevance for.
    keywords : dictionary of string:int pairs
        The keywords to search the string for and the weights given to them.

    Returns
    -------
    float, list :
        The sum of the weights of all keywords in the string, and the keywords
        found.
    """
    filler_words = ('a', 'an', 'and', 'are', 'as', 'at', 'by',
                    'can', "can't", 'cannot', 'did', 'do', "don't",
                    'for', 'from', 'how', 'in', 'is', 'not', 'of', 'over',
                    'the', 'they', 'their', 'to', 'use', 'using',
                    'when', 'which', 'why', 'with')
    relevance = 0

    # Remove filler words from title
    relevant_keywords = []
    title = title.lower().split()
    for word in filler_words:
        while word in title:
            del(title[title.index(word)])
    title = ' '.join(title)

    for keyword in keywords:
        if re.search(keyword, title) is not None:
            relevance += keywords[keyword]
            relevant_keywords.append(keyword)
    return float(relevance) / len(title.split()), relevant_keywords


def split_string_length(string, length):
    """
    Generator, split `string` into strings of length `length`.

    Attempts to break before or after words.
    Useful for displaying a paragraph in the terminal.

    Parameters
    ----------
    string : string
        The string to split.
    length : int
        The maximum length of substrings to return.
    """

    broken = []
    old_pos = 0

    if string == '':
        yield string

    while old_pos < len(string):
        # If this substring is within length 'length', just return it
        if length + old_pos >= len(string):
            yield(string[old_pos:(length + old_pos)])
            break
        # Find the last ' ' in the substring
        new_pos = string[old_pos:(length + old_pos)].rfind(' ', 1)
        if new_pos is -1:  # Didn't reach ' ' - middle of word
            new_pos = length + old_pos
            yield(string[old_pos:new_pos])
            try:
                # If the start of the next substring would be a ' ', skip it
                if string[length + old_pos] == ' ':
                    old_pos = new_pos + 1
                else:
                    raise IndexError
            except IndexError:
                old_pos = new_pos
        else:  # Standard action
            new_pos += old_pos
            yield(string[old_pos:new_pos])
            old_pos = new_pos + 1


def main():
    """
    Accepts and processes command-line arguments and queries arxiv database
    """
    parser = argparse.ArgumentParser(description="Query arXiv for the latest "
                                                 "astro-ph.CO or HE papers and"
                                                 " display the most relevant")
    parser.add_argument('--display-keywords', action='store_true',
                        help='Display the keywords matched in each title')
    parser.add_argument('--he', action='store_true',
                        help='Search astro-ph.HE instead of astro-ph.CO')
    parser.add_argument('--keyword-file',
                        default=(expanduser('~') + '/.arxiv_keywords'),
                        help=('The file of keywords to use '
                              '(default = ~/.arxiv_keywords)'))
    parser.add_argument('--week', action='store_true',
                        help='Only display articles submitted within the '
                             'past week'                                  )
    parser.add_argument('title', nargs='*', default=None,
                        help='A paper title to calculate the weight of and '
                             'list the keywords found within'               )
    args = parser.parse_args()

    # List the keywords and weights
    keywords = {}
    try:
        with open(args.keyword_file) as keyword_file:
            for line in keyword_file.readlines():
                split_line = line.split()
                keywords[split_line[0]] = int(split_line[1])
    except IOError:
        print("The keyword file does not exist, exiting")
        import sys
        sys.exit(1)

    if args.title != []:
        relevance, keywords = title_relevance(' '.join(args.title), keywords)
        print("The title has a relevance of {0:.1f}".format(relevance))
        print("The following keywords were found: {0}".format(keywords))
        return

    # Query arxiv
    entries = []
    for cat in 'CO', 'GA', 'HE':
        url = ('http://export.arxiv.org/api/query?search_query='
               'cat:astro-ph.{}'.format(cat) + ''
               '&start=0'
               '&max_results=500'
               #sortBy can be "relevance", "lastUpdatedDate", "submittedDate"
               '&sortBy=submittedDate'
               '&sortOrder=descending')
        page = ET.fromstring(req.urlopen(url).read())

        for entry in page.iterfind('{http://www.w3.org/2005/Atom}entry'):
            entry_dict = {}
            for child in entry:
                entry_dict[child.tag.split('}')[1]] = child.text
            entries.append(entry_dict)

    unique_entries = []
    for entry in entries:
        if entry['id'] not in (e['id'] for e in unique_entries):
            unique_entries.append(entry)
    entries = unique_entries

    # Get articles
    if args.week:
        articles = [[-1, entry['id'][21:], entry['title'], []]
                    for entry in entries
                    if ((datetime.datetime.now() -
                         datetime.datetime.strptime(entry['published'],
                                               '%Y-%m-%dT%H:%M:%SZ')).days < 7)
                    ]
    else:
        articles = [[-1, entry['id'][21:], entry['title'], []]
                    for entry in entries                      ]

    # Remove latex characters from title
    latex_characters = ('$', '\\', '{', '}')
    for article in articles:
        for character in latex_characters:
            article[2] = article[2].replace(character, '')

    # Sort titles by relevance
    for article in articles:
        article[0], article[3] = title_relevance(article[2], keywords)
    articles.sort(key=lambda article: article[0], reverse=False)

    # Determine terminal width
    import os
    num_cols = int(os.popen('stty size', 'r').read().split()[1])

    # Pretty print results
    for article in articles:
        if article[0] > 1.0:
            # The first part of the output (relevance and arxiv id number)
            relevance_id = '{0:.1f}  {1}'.format(article[0], article[1])
            front_pad = ' ' * len(relevance_id)
            # Format string containing title into several of the correct length
            title_col_width = num_cols - len(relevance_id) - 1
            article[2] = re.sub('\s*\n\s*', ' ', article[2])
            split_title = list(split_string_length(article[2],
                                                   title_col_width))
            print("{0} {1}".format(relevance_id, split_title[0]))
            for title_segment in split_title[1:]:
                print(front_pad, title_segment)
            if args.display_keywords:
                print(front_pad, ' '.join(article[3]))


if __name__ == '__main__':
    main()

